{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Sentiment Analysis on Movie Reviews ü§©\n",
    "\n",
    "Working on this lab should be a **collaborative effort**. We encourage you to work together with your group. If you do not work on your own notebook, make sure you demo to the TA/instructor as a group and share your work across the group after the lab.\n",
    "\n",
    "> Remember to indicate the names of your group members if you use some of the collectively developed code in a future homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "1. Experience the full data science workflow from data aquisition, pre-processing, to building a model and presenting the results. \n",
    "![DSworkflow](utility/pics/DSworkflow.png)\n",
    "2. Work with free-form text data.\n",
    "3. Learn and understand two approaches to sentiment analysis. \n",
    "4. Explore model evaluation techniques and analyze errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "0. [DS Use-Case: Analyzing Customer Feedback](#DS-Use-Case:-Analyzing-Customer-Feedback)\n",
    "1. [Rule-Based Sentiment Prediction](#1.-Rule-Based-Sentiment-Prediction)\n",
    "    1. [Toy Example](#A.-Toy-Example)\n",
    "    2. [Movie Reviews: Test Yourself](#B.-Movie-Reviews:-Test-Yourself)\n",
    "    3. [Evaluation](#C.-Evaluation)\n",
    "2. [Limitations and Introduction to Machine Learning](#2.-Limitations-and-Introduction-to-Machine-Learning)\n",
    "    1. [Quick Introduction to Sentiment Classification and Scikit-Learn](#A.-Quick-Introduction-to-Sentiment-Classification-and-Scikit-Learn)\n",
    "    2. [Coding Task: Evaluate the Sentiment Classifier](#B.-Coding-Task:-Evaluate-the-Sentiment-Classifier)\n",
    "3. [Communicate your Results](#3.-Communicate-your-Results) \n",
    "4. [[Optional] More Things to Try](#[Optional]-More-Things-to-Try)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS Use-Case: Analyzing Customer Feedback\n",
    "\n",
    "Today we want to look at ways to help businesses make the most out of their customers' feedback, which oftentimes comes as textual reviews or comments. To analyze this form of data we can use _sentiment analysis_. It's main goal is to categorize attitudes towards something. This is quite relevant today, Amazon for example, sells products of all kinds; those who purchase these items are able to leave reviews and comments. Besides the ratings that are given (which are often noisy, can easily be created by bots, or are systematically biased), how would a company be able to tell which products are well-liked and which ones should be removed?\n",
    "\n",
    "An easy way is through _sentiment analysis_, where the goal is to predict the sentiment or positivity/negativity of a product or service solely based on the text provided as comments and reviews. In this lab, we will explore two different ways to predict and understand the sentiment of text data. First, we will work through a simple **rule-based algorithm**, looking at positive and negative words to determine the classification of reviews. Following this, we will work through a more sophisticated **machine learning-based approach**, allowing us to _learn_ which words are more commonly found in positive versus negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rule-Based Sentiment Prediction\n",
    "\n",
    "Rule-based sentiment prediction is the easier of the two algorithms to learn and implement. In short, we have a list of positive words and a list of negative words, both of which will be used to calculate a \"sentiment score\" for the review.\n",
    "\n",
    "### A. Toy Example\n",
    "\n",
    "For example, let's say we have two sets of words, positive_words and negative_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = ['great', 'awesome', 'happy', 'good', 'exciting', 'love']\n",
    "negative_words = ['bad', 'dislike', 'sad', 'boring', 'awful', 'poor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a set of reviews or text that we want to analyze. Here we have three example movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['I thought the movie was great! I was very happy I could see it.',\n",
    "           'I did not like the movie; boring acting, poor attitudes, bad lighting.',\n",
    "           'The movie was pretty exciting overall, but the sound quality was bad.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then go through each review and add or subtract to the sentiment score based on the **number of positive** or **negative words**. First, we split the strings by spaces using `split()`. Now, for each word that is in the list of positive words, we add one to the score; for each word in the list of negative words subtract one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = []\n",
    "for review in reviews:\n",
    "    sentiment_score = 0\n",
    "    for word in review.split(' '):\n",
    "        if word in positive_words:\n",
    "            sentiment_score += 1\n",
    "        if word in negative_words:\n",
    "            sentiment_score -= 1\n",
    "    sentiment_scores.append(sentiment_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out these results to see the overall scores in order of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -3, 1]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do this by hand, we see that the scores don't add up correctly. Why is this? When **tokenizing** the reviews into words, we split by spaces. Take the first review for example. If we split it by spaces and look at the words, we see that the word great still has the exclamation point with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'thought', 'the', 'movie', 'was', 'great!', 'I', 'was', 'very', 'happy', 'I', 'could', 'see', 'it.']\n"
     ]
    }
   ],
   "source": [
    "first_review = reviews[0]\n",
    "first_review_words = first_review.split(' ')\n",
    "print(first_review_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the words split only by spaces causes some words to include punctuation, which is something we don't want. We won't touch on this too much, but preprocessing data to make sure words or numbers are functioning correctly can increase performance and accuracy greatly. Making sure that punctuation is removed as well as standardizing to lowercase gives much more control over the text data at hand.\n",
    "\n",
    "**[üêç Python Feature üêç]: String Functions**  \n",
    "\n",
    "We will use some string functions for text preprocessing. Here are a couple of useful examples: \n",
    "\n",
    "> `lower()` changes all characters to lowercase.\n",
    "\n",
    "> `translate(str.maketrans(input, output, delete))` will replace characters from `input` with respective characters in `output` and deletes what's in `delete`. For example `translate(str.maketrans(‚Äúaeiou‚Äù, ‚Äú12345\", \"!\"))` will replace vowels with their respective numbers and deletes all exclamation marks. This is useful if you want to delete or replace a bunch of characters all at once. \n",
    "      \n",
    "> `split(' ')` splits the words into an array based on ' ', or a space.\n",
    " \n",
    "> `replace(target, new)` will replace all matches of the `target` string with the `new` string.\n",
    "\n",
    "> `string.punctuation` gives you all punctuation symbols: $!\"#\\$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~$ \n",
    "You will need to import `string` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'thought', 'the', 'movie', 'was', 'great', 'i', 'was', 'very', 'happy', 'i', 'could', 'see', 'it']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "new_first_words = first_review.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "print(new_first_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Update the tokenization part in the code from above and re-run it on the reviews to see the appropriate scores that should be allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3, 0]\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores = []\n",
    "for review in reviews:\n",
    "    tokens = review.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "        \n",
    "    sentiment_score = 0\n",
    "    for word in tokens: \n",
    "        if word in positive_words:\n",
    "            sentiment_score += 1\n",
    "        if word in negative_words:\n",
    "            sentiment_score -= 1\n",
    "    sentiment_scores.append(sentiment_score)\n",
    "\n",
    "print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have a working function to assign sentiment scores to reviews. The final step is simply to assign a sentiment to the reviews. There are several ways to approach this, depending on what the user is attempting to do. We could do this as a Binary Classification, where each review is either positive or negative, and cannot be anything else. For this, we would assign \"Negative\" to any review with a score less than zero, and \"Positive\" to every other review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Negative', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "review_sentiments = []\n",
    "\n",
    "for score in sentiment_scores:\n",
    "    if score >= 0:\n",
    "        review_sentiments.append(\"Positive\")\n",
    "    if score < 0:\n",
    "        review_sentiments.append(\"Negative\")\n",
    "        \n",
    "print(review_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we could also use Multi-class classification, including a \"Neutral\" class for the reviews that have a score of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Negative', 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "review_sentiments = []\n",
    "\n",
    "for score in sentiment_scores:\n",
    "    if score > 0:\n",
    "        review_sentiments.append(\"Positive\")\n",
    "    if score < 0:\n",
    "        review_sentiments.append(\"Negative\")\n",
    "    if score == 0:\n",
    "        review_sentiments.append(\"Neutral\")\n",
    "        \n",
    "print(review_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of this in mind, there are no limits to the number of classes or splits that could be made for text data. We could adjust the range for neutral to be any reviews between -1 and 1, or perhaps add in more classes (\"Slightly Positive\", \"Slightly Negative\", \"Very Positive\", \"Very Negative\", etc...). \n",
    "> **Caution**: The only challenge with this is that you will need to define the thresholds on the scores. This adds another set of **hard-coded \"rules\"** to your approach. \n",
    "\n",
    "#### Bottom line:\n",
    "\n",
    "* As long as the data is preprocessed correctly and you have a good set of positive and negative words and thresholds, you will be able to run sentiment analysis easily on the majority of text files.\n",
    "\n",
    "\n",
    "* Rule-Based Sentiment Analysis is also _easy to implement_! \n",
    "\n",
    "\n",
    "* But there are several drawbacks that can render this method inefficient. This method fails to correctly handle:\n",
    "    * misspellings\n",
    "    * context\n",
    "    * negations \n",
    "    \n",
    "> Take the two following reviews for example: \"_The movie was not good, it was bad_\" and \"_The movie was not bad, it was good_\" \n",
    "\n",
    "> Both of these reviews would end up with the same sentiment score, but are clearly different reviews. This is partly due to the nature of the method; we are only looking at one word at a time, and not pairs of words. We also didn't implement **negation handling**. We will not look at this specifically, but more elaborate text pre-processing and tokenizing, as well as, looking at pairs of words or groups of three word (called bi-grams or tri-grams or in general n-grams) can help alleviate mistakes in our analysis.\n",
    "\n",
    "* Rule-Based Sentiment Analysis also does not take into account the length of the review. If we have a very long review that uses a mix of positive and negative words, it may end up being classified as something it is not. Likewise, a short but very strongly opinionated review may not receive the same sentiment as a longer, equally opinionated review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Movie Reviews: Test Yourself\n",
    "\n",
    "In the following code blocks, work through them to analyze a dataset of real-life **movie reviews**! Some of the code is written for you and some you will have to fill in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped Negative\n",
      "Unzipped Positive\n",
      "Created test folder.\n",
      "Created list of negtaive words: negative_words\n",
      "Created list of postive words: postitive_words\n"
     ]
    }
   ],
   "source": [
    "# Setup - This cell block is needed to set up everything for this testing section\n",
    "# No need to edit this cell\n",
    "\n",
    "import os\n",
    "import string\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Unzip folder with negative reviews\n",
    "if not os.path.exists('utility/data/neg'):\n",
    "    zip_ref = zipfile.ZipFile('utility/data/neg.zip', 'r')\n",
    "    zip_ref.extractall('utility/data/')\n",
    "    zip_ref.close()\n",
    "    print('Unzipped Negative')\n",
    "\n",
    "# Unzip folder with postive reviews\n",
    "if not os.path.exists('utility/data/pos'):\n",
    "    zip_ref = zipfile.ZipFile('utility/data/pos.zip', 'r')\n",
    "    zip_ref.extractall('utility/data/')\n",
    "    zip_ref.close()\n",
    "    print('Unzipped Positive')\n",
    "    \n",
    "# Create folder for testing\n",
    "pos_test = ['357_10p.txt', '347_10p.txt', '1697_10p.txt', '13_10p.txt']  \n",
    "neg_test = ['1919_1n.txt', '54_1n.txt', '1819_1n.txt', '7_1n.txt'] \n",
    "\n",
    "if not os.path.exists('utility/data/test'):\n",
    "    os.mkdir('utility/data/test')\n",
    "    \n",
    "    for rev in pos_test:\n",
    "        shutil.copy('utility/data/pos/'+rev,'utility/data/test')\n",
    "\n",
    "    for rev in neg_test:\n",
    "        shutil.copy('utility/data/neg/'+rev,'utility/data/test')\n",
    "    print('Created test folder.')\n",
    "    \n",
    "# Create list of positive words from given file\n",
    "with open('utility/data/negative-words.txt') as f:\n",
    "    negative_words = [word.strip() for word in f.readlines() if word[0] not in [';', '\\n']]\n",
    "    print('Created list of negtaive words: negative_words')\n",
    "\n",
    "# Create list of negative words from given file\n",
    "with open('utility/data/positive-words.txt') as f:\n",
    "    positive_words = [word.strip() for word in f.readlines() if word[0] not in [';', '\\n']]\n",
    "    print('Created list of postive words: postitive_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** The bulk of the code will be executed in the following function. Fill in what needs to be filled in to perform rule-based sentiment prediction and test the function on a small number of reviews. \n",
    "\n",
    "> **[üêç Python Feature üêç] Reading text from files:** There are different ways to read data from text files: next to `f.readlines()`, there is `f.readline()` and also `f.read()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...computing sentiment scores on utility/data/test...\n",
      "Done Running \n",
      "\n",
      "[-3, 2, -2, -2, -6, 5, 6, -19]\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment_scores(path2folder,test_mode=False):\n",
    "\n",
    "    # Create a blank sentiment_scores list\n",
    "    sentiment_scores = []\n",
    "\n",
    "    print('...computing sentiment scores on '+path2folder +'...')\n",
    "    \n",
    "    # get the filenames \n",
    "    testfiles = os.listdir(path2folder)\n",
    "    \n",
    "    # sort test files (only in test-mode) \n",
    "    if test_mode:\n",
    "        testfiles.sort()\n",
    "    \n",
    "    \n",
    "    for file in testfiles:\n",
    "        \n",
    "        path_start = path2folder + '/'\n",
    "    \n",
    "        # Create the sentiment_score variable for this review, and set it to zero\n",
    "        sentiment_score = 0\n",
    "        # your code here \n",
    "\n",
    "    \n",
    "        with open(path_start + file, encoding = \"utf-8\") as f:\n",
    "            \n",
    "            words = f.read().lower().replace(\"<br />\", \" \").translate(str.maketrans(\"\", \"\", string.punctuation)).split(\" \")\n",
    "            \n",
    "\n",
    "            # Pull the words into a words array\n",
    "\n",
    "            # The reviews include the string \"<br />\" quite a few times; the data looks cleaner if replaced\n",
    "            # with a space!\n",
    "            \n",
    "\n",
    "            # Hint: Remember to read, lower, replace, translate, and split!\n",
    "\n",
    "            # your code here \n",
    "\n",
    "\n",
    "            # Loop through the words to generate the sentiment score\n",
    "            \n",
    "            for word in words: \n",
    "                if word in positive_words:\n",
    "                    sentiment_score += 1\n",
    "                if word in negative_words:\n",
    "                    sentiment_score -= 1\n",
    "            sentiment_scores.append(sentiment_score)\n",
    "    \n",
    "            # your code here \n",
    "\n",
    "\n",
    "            # Append the sentiment_score to the sentiment_scores array!\n",
    "\n",
    "            # your code here \n",
    "\n",
    "        \n",
    "    print('Done Running \\n')\n",
    "    return sentiment_scores\n",
    "\n",
    "# executing in test-mode on test folder\n",
    "test_scores = get_sentiment_scores('utility/data/test',test_mode=True)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Check your Code**: The sentiment scores you should get for those test reviews are `[-3, 2, -2, -2, -6, 5, 6, -19]` (note: they may not be in this order, but as long as the same scores are present you should be good)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Once the code is running correctly, perform rule-based sentiment prediction by calling `get_sentiment_scores()` on all the **_positive reviews_**! Running this function will take a little while as it needs to go through all of the reviews and count the positive and negative words in order to get the sentiment score.  \n",
    "\n",
    "> **Hint**: The data is stored in the folder `data` under `utility`, with two subfolders being `neg` or `pos`\n",
    "\n",
    "> **Hint**: Provide only the path as an argument to `get_sentiment_scores()` (to trigger _non-test mode_ execution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...computing sentiment scores on utility/data/pos...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c0263c687b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscores_pos_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentiment_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utility/data/pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-6ddfb25d10f8>\u001b[0m in \u001b[0;36mget_sentiment_scores\u001b[0;34m(path2folder, test_mode)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0msentiment_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     \u001b[0msentiment_score\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0msentiment_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores_pos_reviews = None\n",
    "\n",
    "# your code here \n",
    "scores_pos_reviews = get_sentiment_scores('utility/data/pos')            \n",
    "\n",
    "\n",
    "len(scores_pos_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Repeat the sentiment score computation for all **_negative reviews_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...computing sentiment scores on utility/data/neg...\n",
      "Done Running \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_neg_reviews = None\n",
    "\n",
    "# your code here \n",
    "scores_neg_reviews = get_sentiment_scores('utility/data/neg')\n",
    "\n",
    "len(scores_neg_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Evaluation\n",
    "Now, we can see how our approach predicts the sentiment for those reviews. This phase is a crucial part in the data science workflow as it will tell us how well our model or approach works.  \n",
    "\n",
    "#### Accuracy \n",
    "What is the overall performance of our rule-based sentiment predictor? \n",
    "\n",
    "**Try this!** Compute the percentage of correctly predicted reviews over *all* reviews (this measure is also called _accuracy_), and the percentage of incorrectly predicted reviews over *all* reviews (this measure is also called _error rate_). As a sanity check, make sure both measures add up to 100%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE DATA\n",
      "percent correct: \n",
      "79.1\n",
      "percent incorrect\n",
      "20.9\n",
      "NEGATIVE DATA\n",
      "percent correct: \n",
      "72.7\n",
      "percent incorrect\n",
      "27.3\n"
     ]
    }
   ],
   "source": [
    "# your code here \n",
    "\n",
    "num_true = 0\n",
    "num_false = 0\n",
    "for i in range(len(scores_pos_reviews)): \n",
    "    if scores_pos_reviews[i] > 0: \n",
    "        num_true+=1\n",
    "    else: \n",
    "        num_false +=1\n",
    "        \n",
    "print(\"POSITIVE DATA\")\n",
    "print(\"percent correct: \")\n",
    "print(100*num_true/len(scores_pos_reviews))\n",
    "\n",
    "print(\"percent incorrect\")\n",
    "print(100*num_false/len(scores_pos_reviews))\n",
    "    \n",
    "neg_true = 0\n",
    "neg_false = 0\n",
    "for i in range(len(scores_neg_reviews)): \n",
    "    if scores_neg_reviews[i] < 0: \n",
    "        neg_true+=1\n",
    "    else: \n",
    "        neg_false +=1\n",
    "print(\"NEGATIVE DATA\")\n",
    "print(\"percent correct: \")\n",
    "print(100*neg_true/len(scores_neg_reviews))\n",
    "\n",
    "print(\"percent incorrect\")\n",
    "print(100*neg_false/len(scores_neg_reviews))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [üêç Python Feature üêç] Quick Intro to Formatted Printing:\n",
    "> **ProTip**: Use _formatted printing_ to get nice print statements and save yourself from doing `str` conversions all the time.  \n",
    "\n",
    "> The general rules are: `%[flags][width][.precision]type`, where `%` indicates that we want to format something at this point in the string.  Then you need to add the comma-separated variables you want to format surrounded by `(` `)` after the string preceeded by another `%`. You can add multiple inputs that get fomatted in differnt locations in your string that way. \n",
    "\n",
    "Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class :     1, Score :  0.33\n"
     ]
    }
   ],
   "source": [
    "my_sentiment = 1\n",
    "my_score = 0.33333\n",
    "print(\"Class : %5d, Score : %5.2f\" % (my_sentiment, my_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break it down: `d` means that the first input is trated as integer and the `5` in both formatting instructions means to use a width of 5 characters (even if the displayed string is smaller) and `.2f` means to include 2 decimal places and to treat the input as a `float` (we did not use `flags`). \n",
    "\n",
    "\n",
    "> **Note**: the `%%` in the cell below escapes the '%' symbol that we  want to print out.\n",
    "\n",
    "\n",
    "#### True Positives, False Positives, True Negatives, and False Negatives\n",
    "\n",
    "Let's look at the different poissble errors we can make on positve versus negative reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.40% true positive reviews (those are predicted correctly)\n",
      "15.60% false negative reviews (those are actually positive reviews)\n"
     ]
    }
   ],
   "source": [
    "# Positive Predicted Reviews:\n",
    "percent_pos = sum([1 for score in scores_pos_reviews if score >= 0]) / len(scores_pos_reviews)*100\n",
    "print(\"%.2f%% true positive reviews (those are predicted correctly)\" % (percent_pos))\n",
    "\n",
    "# Negative Predicted Reviews:\n",
    "percent_neg = sum([1 for score in scores_pos_reviews if score < 0]) / len(scores_pos_reviews)*100\n",
    "print(\"%.2f%% false negative reviews (those are actually positive reviews)\" % (percent_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these numbers mean? Explain whether our approach works well or not.\n",
    "\n",
    "Let's look at the negative reviews: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.30% false positive reviews (those are actually negtaive reviews)\n",
      "72.70% true negative reviews (those are predicted correctly)\n"
     ]
    }
   ],
   "source": [
    "# Positive Predcited Reviews:\n",
    "percent_pos = sum([1 for score in scores_neg_reviews if score >= 0]) / len(scores_neg_reviews)*100\n",
    "print(\"%.2f%% false positive reviews (those are actually negtaive reviews)\" % (percent_pos))\n",
    "\n",
    "# Negative Predicted Reviews:\n",
    "percent_neg = sum([1 for score in scores_neg_reviews if score < 0]) / len(scores_neg_reviews)*100\n",
    "print(\"%.2f%% true negative reviews (those are predicted correctly)\" % (percent_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Good to Know:** The error values we computed are the standard way to evaluate binary classification models. The values can be summarized in what is called the _confusion matrix_: \n",
    "\n",
    "<img src=\"utility/pics/confusion_matrix.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "**Write-up!** Compare the results for negative reviews with the ones for poistive ones above. Is our approach better in predicting positive reviews correctly or negative ones? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response here:**\n",
    "\n",
    "our approach is better at predicting positive reviews. \n",
    "(this is confusing, but easier to think about when you put the false/true values on the chart above^^)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limitations and an Introduction to Machine Learning\n",
    "The rule-based sentiment predictor has many advantages, like being so simple to implement. With just a couple of extensions to our version (such as negation handling) we could actually make this production ready. However, the main drawback of this approach is that we need **hand engineered** lists of positive and negative expressions, which are non-trivial to create and also static. That means they don't adapt automatically to the domain they are being used for. For example, formal language expressions might have different meanings when compared to a colloquial context. \n",
    "\n",
    "#### Rule-Based: \n",
    "![rule-based](utility/pics/rule-based1.png)\n",
    "\n",
    "How can we overcome this problem? Can we maybe learn what expressions are used in a positive versus a negative review? The answer is '_yes - we can!_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Quick Introduction to Sentiment Classification and Scikit-Learn\n",
    "\n",
    "Instead of working with lists of positive and negative expressions we will now look at reviews with known ratings and use them to learn what positive versus negative reviews are. With the known set of positive and negative reviews, we can build a model just like so: \n",
    "\n",
    "#### Training an ML approach:\n",
    "![machine-learning](utility/pics/ml_train1.png)\n",
    "\n",
    "And then use it on new comments and reviews to determine a customer's attitudes. This approach is a **machine learning** approach commonly known as _classification_. Just like so: \n",
    "\n",
    "#### Use Trained Classifier:\n",
    "![predict](utility/pics/ml_predict1.png)\n",
    "\n",
    "Okay, let's do it. We will be using the Scikit-Learn `sklearn` Python package (https://scikit-learn.org/stable/). For further reading/reference, check [**PDSH**] Ch5 for a quick introduction to Scikit-Learn (p343-359).  \n",
    "\n",
    "> It's okay to **not understand** every single statement we are doing in the following cells. Just enjoy the flow and watch how things evolve. Concentrate on the output and the bigger picture: which method is better _rule-based_ or _ML-based_? \n",
    "\n",
    "\n",
    "> **We will spend a lot more time on machine learnign and the** `sklearn` **library in the upcoming weeks!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 4000\n",
      "Number of words: 8870\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Remove test folder \n",
    "if os.path.exists('utility/data/test'):\n",
    "    shutil.rmtree('utility/data/test')  \n",
    "    \n",
    "# Load data (folders will be considered as classes (target variable) 0,1,... # subfolders)    \n",
    "data_folder = \"utility/data/\"\n",
    "dataset = load_files(data_folder, shuffle=False)\n",
    "docs_raw = dataset.data\n",
    "\n",
    "## Text preprocessing\n",
    "docs_all = []\n",
    "for doc in docs_raw:\n",
    "    docs_all.append(doc.decode('utf-8', errors='replace')) # prevent UnicodeDecodeError\n",
    "y_all = dataset.target\n",
    "\n",
    "# Text tokenizing and filtering of stopwords\n",
    "count_vect = CountVectorizer(min_df=5)  \n",
    "X_all_counts = count_vect.fit_transform(docs_all)\n",
    "\n",
    "# Number of docs and number of words\n",
    "print(\"Number of documents: \" + str(X_all_counts.shape[0])) \n",
    "print(\"Number of words: \" + str(X_all_counts.shape[1])) \n",
    "    # X_all_counts data representation (* = occurrence count):\n",
    "    #    - - - - -\n",
    "    #  |\n",
    "    #  |  *        <- document\n",
    "    #  |\n",
    "    #  |\n",
    "    #     ^\n",
    "    #    word index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After **preprocessing** the text documents, we **split** our data into two parts: one for building the model (_training set_) \n",
    "and one for testing/evaluating it (_test/evaluation set_). Then we will **build the model** using the _training set_ and use the model to **predict** the sentiment of the documents in the _testing set_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training set: 3200\n",
      "Size of the test/evaluation set: 800\n"
     ]
    }
   ],
   "source": [
    "# Split the data into two parts \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_counts, y_all, train_size = .8, test_size = .2, random_state = 16)\n",
    "\n",
    "print(\"Size of the training set: \" + str(X_train.shape[0]))\n",
    "print(\"Size of the test/evaluation set: \" + str(X_test.shape[0]))\n",
    "\n",
    "# Build the model using a linear classification model\n",
    "model = LogisticRegression(max_iter=1000).fit(X_train,y_train)\n",
    "\n",
    "# Use the classification model for predictions\n",
    "predicted_target = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Coding Task: Evaluate the Sentiment Classifier\n",
    "Write a function that will go through all the test data and compare the predicted class and the actual class. If an entry is put into the wrong class by the model, this function will add one to the respective variable: `fneg_error_count` if it was a _false negative_, `fpos_error_count` if it is a _false positive_. From these values you can compute  \n",
    "* the total _number of mistakes made_, \n",
    "* the _error rate_, and \n",
    "* the _accuracy_ \n",
    "\n",
    "of the machine learning approach. \n",
    "\n",
    "\n",
    "Then, this function will print out how many total errors, how many _false negatives_, and how many _false positives_ were found and the rates (which important to get the relative measure based on the number of positive/negtaive test examples). \n",
    "\n",
    "The inputs for this function are the **predicted classifications** for each review generated by the model and the **actual classifications** from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions(predictions, actual):\n",
    "    \n",
    "    fneg_error_count = 0\n",
    "    fpos_error_count = 0\n",
    "\n",
    "    mistakes = 0\n",
    "    error_rate = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    num_pos = np.sum(actual==1)\n",
    "    num_neg = np.sum(actual==0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # your code here \n",
    "    for i in range(len(actual)): \n",
    "        if predictions[i] != actual[i]: \n",
    "            mistakes +=1\n",
    "            if actual[i]==1: \n",
    "                fneg_error_count +=1\n",
    "            else: \n",
    "                fpos_error_count +=1\n",
    "        else: \n",
    "            accuracy +=1 \n",
    "            \n",
    "\n",
    "    error_rate = 100*mistakes/len(actual)\n",
    "    accuracy = 100*accuracy/len(actual)\n",
    "            \n",
    "\n",
    "    \n",
    "    print(\"There were \" + str(fneg_error_count) + \" false negative errors\")\n",
    "    print(\"There were \" + str(fpos_error_count) + \" false positive errors\")\n",
    "    print(\"There were a total of \" + str(mistakes) + \" errors out of \" + str(len(predictions)) + \" testpoints.\\n\")\n",
    "    \n",
    "    \n",
    "    #false negative and false postive rates\n",
    "    fnr = fneg_error_count/num_pos *100\n",
    "    fpr = fpos_error_count/num_neg *100\n",
    "    \n",
    "    print(\"%5.2f%% true positive reviews (those are predicted correctly)\" % (100-fnr))\n",
    "    print(\"%5.2f%% false negative reviews (those are actually positive reviews)\" % (fnr))\n",
    "    \n",
    "    print(\"%4.2f%% false positive reviews (those are actually negtaive reviews)\" % (fpr))\n",
    "    print(\"%5.2f%% true negative reviews (those are predicted correctly)\\n\" % (100-fpr))\n",
    "    \n",
    "    print(\"The algorithm was correct in %.2f%% of the test cases.\" % (accuracy) )\n",
    "    print(\"The algorithm was wrong in %.2f%% of the test cases.\" % (error_rate) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call this function using our predicted sentiments and the ground truth sentiments as input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 30 false negative errors\n",
      "There were 47 false positive errors\n",
      "There were a total of 77 errors out of 800 testpoints.\n",
      "\n",
      "92.27% true positive reviews (those are predicted correctly)\n",
      " 7.73% false negative reviews (those are actually positive reviews)\n",
      "11.41% false positive reviews (those are actually negtaive reviews)\n",
      "88.59% true negative reviews (those are predicted correctly)\n",
      "\n",
      "The algorithm was correct in 90.38% of the test cases.\n",
      "The algorithm was wrong in 9.62% of the test cases.\n"
     ]
    }
   ],
   "source": [
    "test_predictions(predicted_target, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Communicate your Results\n",
    "\n",
    "Now, it is time to summarize your findings by comparing the two approaches and their results, as well as, to criticallly review your work. Did you find a satisfying solution to the given business problem: _analyzing customer feedback_? What are the strengths and limitations of the approach(es)? What could be imporved in the future?  \n",
    "\n",
    "**Group Discussion:** Compare the two apporaches **rule-based sentiment prediction** versus **sentiment classification**. What are the main differences in terms of... \n",
    "* required data?\n",
    "* quality of the results? \n",
    "* efficiency of the computation?\n",
    "* possibilities to extend the basic algorithms? \n",
    "\n",
    "**Write-up!** Complete the given table: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| .             | required data | results    | efficiency | extensions | other things\n",
    "| ---           | ---           | ---        | ---        | ---        | ---\n",
    "| rule-based SA |pos/neg list+reviews  |  label          |   ok         |    give arrays        |\n",
    "| ML-based SA   |new reviews/model |     label       |     more efficient       |    training approach        |  this one is probably better  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write-up!** Collect the main _pros_ and _cons_ for both approaches and goive a recommendation on which one to use to analyze your company's customer feedback data. This will likley be very helpful for  the decision makers discussing the integration of SA into your company's business processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your response here:** \n",
    "\n",
    "ML \n",
    "pros: more accurate\n",
    "cons: need more data\n",
    "\n",
    "rule based\n",
    "pros: small set of data needed \n",
    "cons: less accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] More Things to Try\n",
    "\n",
    "There are a lot of (ad-hoc) decisions we have made for you with repect to the machine leanning pipeline above. We encourage you to modify some of these to see if and how the results will be affected. E.g.,\n",
    "\n",
    "* Play with the train/test split sizes: we used a 80/20 split, but you can change this and see if it has an effect on the results. \n",
    "* Play with the random seed to create different train/test splits. How does this affect the results? \n",
    "* Use a different classifier: for example, NaiveBayes or a Support Vector Machine (SVM). Code examples are below - **replace** the model computation in the cell above with the respective lines to train these different models. Do these models produce different (better/worse) results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(max_iter=5000).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it turns out that this performs quite well. Of course, we can do more fancy things with the text data, instead of only counting word occurrences. \n",
    "\n",
    "[**Challenge**] In practice people also use the counts of _pairs of words_ (so-called _bi-grams_) or even _n-grams_ (counts of tuples of n words), or a feature called _TF-IDF_, which is very powerful in practice. If you still have time, check-out this tutorial explaining how to compute those: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html. Adapt the features used, create a new train/test split, train the model again, and evaluate the performace using your new features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up\n",
    "Please run the following cell in order to clean up some of the files on your computer. While not mandatory, it will certainly save some space (over 4000 files are already unzipped, this will clear space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to clean folders (unless you want to keep several thousand text files on your computer!)\n",
    "\n",
    "if os.path.exists('utility/data/neg'):\n",
    "    shutil.rmtree('utility/data/neg')\n",
    "if os.path.exists('utility/data/pos'):\n",
    "    shutil.rmtree('utility/data/pos')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
